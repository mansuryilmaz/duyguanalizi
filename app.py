import streamlit as st
import pandas as pd
import tweepy
import re
import string
import torch
import torch.nn.functional as F
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import matplotlib.pyplot as plt
import time
import emoji
import os
from dotenv import load_dotenv

# --- 1. ENV DosyasÄ±nÄ± YÃ¼kle ---
load_dotenv()

# --- 2. BEARER TOKEN ---
BEARER_TOKEN = os.getenv("BEARER_TOKEN")

# --- 3. TÃ¼rkÃ§e Duygu Modeli ---
MODEL = "savasy/bert-base-turkish-sentiment-cased"
try:
    @st.cache_resource
    def load_model():
        tokenizer = AutoTokenizer.from_pretrained(MODEL)
        model = AutoModelForSequenceClassification.from_pretrained(MODEL)
        return tokenizer, model
    
    tokenizer, model = load_model()
    labels = ["Olumsuz", "NÃ¶tr", "Olumlu"]
    MODEL_LOADED = True
except Exception as e:
    st.error(f"âš ï¸ Model yÃ¼klenirken hata: {e}")
    MODEL_LOADED = False

# --- 4. Twitter API BaÄŸlantÄ±sÄ± ---
try:
    if BEARER_TOKEN:
        client = tweepy.Client(bearer_token=BEARER_TOKEN)
        CLIENT_CONNECTED = True
    else:
        st.error("âš ï¸ Bearer Token tanÄ±mlanmadÄ± veya .env dosyasÄ±ndan okunamadÄ±.")
        CLIENT_CONNECTED = False
except Exception as e:
    st.error(f"âŒ API baÄŸlantÄ± hatasÄ±: {e}")
    CLIENT_CONNECTED = False

# --- 5. Temizleme Fonksiyonu (Emoji dahil) ---
def clean_tweet(text):
    if pd.isna(text):
        return ""
    
    text = str(text).lower()
    text = re.sub(r"http\S+", "", text)
    text = re.sub(r"@\w+", "", text)
    text = re.sub(r"#", "", text)
    text = re.sub(r"[0-9]+", "", text)
    text = text.translate(str.maketrans("", "", string.punctuation))
    
    # Emoji temizleme
    text = emoji.replace_emoji(text, replace="")
    
    text = re.sub(r"\s+", " ", text).strip()
    return text

# --- 6. Duygu Tahmini ---
def predict_sentiment(text):
    if not MODEL_LOADED or not text:
        return "NÃ¶tr"
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True)
    with torch.no_grad():
        outputs = model(**inputs)
    probs = F.softmax(outputs.logits, dim=-1)
    return labels[torch.argmax(probs).item()]

# --- 7. Tweet Ã‡ekme ---
@st.cache_data(ttl=600)
def fetch_tweets(keyword, count):
    if not CLIENT_CONNECTED:
        st.error("API baÄŸlantÄ±sÄ± yok.")
        return None

    tweets_data = []
    max_count = min(count, 100)
    for attempt in range(3):
        try:
            response = client.search_recent_tweets(
                query=keyword + " lang:tr -is:retweet",
                max_results=max_count,
                tweet_fields=["created_at", "public_metrics"]
            )
            if response.data:
                for tweet in response.data:
                    tweets_data.append({
                        "Konu": keyword,
                        "tweet": tweet.text,
                        "RT_SayÄ±sÄ±": tweet.public_metrics.get("retweet_count", 0),
                        "BeÄŸeni_SayÄ±sÄ±": tweet.public_metrics.get("like_count", 0)
                    })
                return pd.DataFrame(tweets_data)
            else:
                return pd.DataFrame()
        except tweepy.errors.TooManyRequests:
            time.sleep(60)
        except Exception as e:
            st.error(f"Hata: {e}")
            return None
    st.error("Veri Ã§ekilemedi.")
    return None

# --- 8. Streamlit ArayÃ¼zÃ¼ ---
st.set_page_config(page_title="Sosyal Medya Duygu Analizi", layout="wide")
st.title("ğŸ“Š TÃ¼rkÃ§e Duygu Analizi (Dosya Kaydetmeli)")

keyword = st.text_input("Analiz edilecek konu veya hashtag:", "Ä°NÃ–NÃœ ÃœNÄ°VERSÄ°TESÄ°")
tweet_limit = st.slider("KaÃ§ tweet Ã§ekilsin?", 10, 100, 20)

# 1ï¸âƒ£ Tweetleri Ã§ek ve kaydet
if st.button("ğŸ“¥ Tweetleri Ã‡ek ve Kaydet"):
    if not keyword:
        st.warning("Bir konu giriniz.")
        st.stop()
    if not CLIENT_CONNECTED:
        st.warning("API'ye baÄŸlanÄ±lamÄ±yor.")
        st.stop()

    with st.spinner(f"â€˜{keyword}â€™ ile ilgili tweetler Ã§ekiliyor..."):
        df = fetch_tweets(keyword, tweet_limit)
    if df is None or df.empty:
        st.error("Tweet bulunamadÄ±.")
        st.stop()

    df.to_csv("tweets.csv", index=False, encoding="utf-8-sig")
    st.success(f"{len(df)} tweet Ã§ekildi ve kaydedildi âœ…")
    st.dataframe(df, use_container_width=True, height=500)

    csv_bytes = df.to_csv(index=False, encoding="utf-8-sig").encode('utf-8')
    st.download_button(
        label="â¬‡ï¸ Tweetleri CSV Olarak Ä°ndir",
        data=csv_bytes,
        file_name="tweets.csv",
        mime="text/csv"
    )

# 2ï¸âƒ£ Kaydedilen CSVâ€™yi modele ver ve analiz et
st.markdown("---")
st.subheader("ğŸ¤– Kaydedilen DosyayÄ± Model ile Analiz Et")
if st.button("â–¶ï¸ Analiz Et (tweets.csv dosyasÄ±ndan)"):

    try:
        df = pd.read_csv("tweets.csv")
    except FileNotFoundError:
        st.error("Ã–nce tweetleri Ã§ekip kaydetmelisin.")
        st.stop()

    df["clean_tweet"] = df["tweet"].apply(clean_tweet)
    df["Duygu"] = df["clean_tweet"].apply(predict_sentiment)

    df.to_csv("results.csv", index=False, encoding="utf-8-sig")
    st.success("Analiz tamamlandÄ± âœ…")
    st.dataframe(df, use_container_width=True, height=500)

    csv_bytes = df.to_csv(index=False, encoding="utf-8-sig").encode('utf-8')
    st.download_button(
        label="â¬‡ï¸ Analiz SonuÃ§larÄ±nÄ± CSV Olarak Ä°ndir",
        data=csv_bytes,
        file_name="results.csv",
        mime="text/csv"
    )

    # Grafiksel GÃ¶sterim (Pasta GrafiÄŸi)
    st.subheader("ğŸ“Š Duygu DaÄŸÄ±lÄ±mÄ±")
    counts = df["Duygu"].value_counts()
    fig, ax = plt.subplots()
    ax.pie(
        counts,
        labels=counts.index,
        autopct="%1.1f%%",
        startangle=90,
        colors=["red", "gray", "green"]
    )
    ax.axis("equal")  # Daire ÅŸeklinde gÃ¶stermek iÃ§in
    st.pyplot(fig)
